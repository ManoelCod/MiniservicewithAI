# ğŸ’¬ MiniservicewithAI

Um microserviÃ§o inteligente que responde automaticamente a mensagens de clientes sobre dÃ©bitos, utilizando IA com TinyLlama, Redis para fila de mensagens e SQLite para histÃ³rico.

## âœ… Requisitos

Antes de executar a aplicaÃ§Ã£o, certifique-se de ter os seguintes componentes instalados:

### ğŸ§° DependÃªncias locais

- [Python 3.10+](https://www.python.org/)
- [pip](https://pip.pypa.io/)
- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)

### ServiÃ§o em segundo plano

docker run -d --name miniservice_redis -p 6379:6379 redis:7

### Ollama com TinyLlama
docker run -d --name miniservice_ollama -p 11434:11434 ollama/ollama

Depois de subir o container, entre nele e ative o modelo:
docker exec -it miniservice_ollama bash
ollama run tinyllama

## ğŸš€ Funcionalidades

- ğŸ” Enfileiramento de mensagens com Redis
- ğŸ¤– Respostas automÃ¡ticas com IA (TinyLlama via Ollama)
- ğŸ§¾ Consulta de dÃ©bitos por nÃºmero de telefone
- ğŸ—ƒï¸ Armazenamento de histÃ³rico e erros em SQLite
- ğŸ“Š DocumentaÃ§Ã£o interativa com Swagger (Flasgger)
- ğŸ› ï¸ Worker assÃ­ncrono para processar mensagens em segundo plano

---

## ğŸ§± Estrutura do Projeto


---

## âš™ï¸ Como executar

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/MiniservicewithAI.git
cd MiniservicewithAI

pip install -r requirements.txt

python -c "from app.storage import init_db; init_db()"

4. Inicie o servidor Flask
python run.py

Inicie o worker 
python -m app.worker





